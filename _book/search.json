[
  {
    "objectID": "notebooks/data_exploration.html#questions",
    "href": "notebooks/data_exploration.html#questions",
    "title": "2  GDPR Fines Data Exploration",
    "section": "2.1 Questions",
    "text": "2.1 Questions\nThrough visualisation the following questions will be explored. This is with the goal of understanding: The financial penalty (fine amount), poor data controllers, particular articles.\nOn the Fine Amount: \n\nHow are the fine amounts distributed?\nHow does the distribution of fine amount change over time?\n\nOn the Controllers: \n\nWho pays the most?\nWho pays the most often?\n\nOn the Articles: \n\nWhich articles are referenced the most often\nWhat is the distribution of fine amounts where that article is referenced?\n\n\n2.1.1 Fine Value\nQuestions on the fine amount:\n\nHow are the fine amounts distributed?\nHow does the distribution of fine amount change over time?\n\n\nHow are the fines distributed?\n\n\nCode\ndef big_currency(x, pos):\n    \"\"\"Lots of big amounts paid in fines. (&gt; 1 billion sometimes)\"\"\"\n    if x &gt;= 1e9:\n        return '{:1.1f}B €'.format(x*1e-9)\n    if x &gt;= 1e6:\n        return '{:1.0f}M €'.format(x*1e-6)\n    \n    return '{:1.0f} €'.format(x)\n\n\n\n\nCode\nfig, ax = plt.subplots()\n\nsns.histplot(\n    fines_nozero, x=\"price\",\n    edgecolor=\"white\", linewidth=.5, facecolor=\"midnightblue\",\n    log_scale=True, binrange=(0, 10), bins=20,\n    ax=ax\n)\nfor bar in ax.patches:\n    bar.set_alpha(0.70) \n\nax.set(xlabel= None, ylabel=None)\nax.xaxis.set_major_formatter(big_currency)\n\nfig.supylabel(t=\"GDPR Violations\")\nfig.supxlabel(t=\"GDPR Fine (Euro)\")\n\nfig.canvas.draw()\n\nfig.suptitle(\n    t=\"GDPR Fine Distributions (Log Scale)\", \n    x=ax.get_position().x0,\n    y=ax.get_position().y1,\n    ha=\"left\", fontsize=16,\n)\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nBy visualising the distribution of fines as a simple histogram it was immediately clear there was a large degree of right-skew. This is typical with financial data. Because of the high right-skew, the decision was made to visualise the data on a logarithmic scale.\nTable 2.3 shows the five figure summary of the price column. Fine value ranges from 28 to 1,200,000,000.\nMost of the fines fall between 1000 and 100,000 Euros.\n\n\nHow does the distribution of fines change over time?\nLooking at each year’s distribution of fines as a boxplot allows for them to be compared. If they boxes move up the y-axis it would give credence that the fine values are increasing over time.\nBy including the actual data points (each fine occurence) as an overlayed jitters, it’s also possible to see if the number of fines is increasing.\n\n\nCode\ndf = (fines_nozero.assign(\n    year = lambda x: x.date.dt.year\n))\n\n# make data more friendly for matplotlib\nprices_each_year = df.groupby('year')['price'].apply(lambda x: x.values)\n\n# Get the years as labels\nlabels = list(prices_each_year.keys())\nlabels.sort()\n\n#create a figure with subplots\nfig, ax = plt.subplots(1, 2,\n                       gridspec_kw={'width_ratios': [1, 7]},\n                       sharey=True)\n\n# Create a boxplot for the year 1970\nbox1 = ax[0].boxplot(\n    prices_each_year[1970],\n    labels=['1970'], widths=0.4, showfliers=False,\n    medianprops={'color':'black', 'linewidth':2}\n    )\nax[0].set_ylabel(None)\n\n\n# Create a boxplot for the years other than 1970\nyears_other_than_1970 = [year for year in labels if year != 1970]\nbox2 = ax[1].boxplot(\n    [prices_each_year[year] for year in years_other_than_1970],\n    labels=years_other_than_1970, showfliers=False, widths=0.4,\n    medianprops={'color':'black', 'linewidth':2}\n    )\n\n# smaller caps\nfor box in list((box1, box2)):\n    for cap in box['caps']:\n        cap.set_xdata(cap.get_xdata() - [-0.05, 0.05])\n\nax[1].set_ylabel(None)\n\njittered_x = np.random.normal(0 + 1, 0.1, len(prices_each_year[1970]))\nax[0].scatter(jittered_x, prices_each_year[1970], alpha=0.05, color='midnightblue', s=10)\n\n\n# Overlay the data points for the years other than 1970\nfor i, year in enumerate(years_other_than_1970):\n    jittered_x = np.random.normal(i + 1, 0.1, len(prices_each_year[year]))\n    ax[1].scatter(jittered_x, prices_each_year[year], alpha=0.05, color='midnightblue', s=10)\n\n# delineate the year jump\nax[1].spines.left.set_visible(True)\nax[1].set_yscale('log')\nax[1].yaxis.set_major_formatter(big_currency)\n\n# draw the figure to apply constrained_layout (req for positioning suptitle)\nfig.canvas.draw()\n\nfig.supylabel(\"Price [Log Distribution]\")\nfig.suptitle(\n    t=\"How has the distribution of fines changed over time?\", \n    ha='left',\n    x=ax[0].get_position().x0+0.03,\n    fontsize=16,\n    y=ax[0].get_position().y1*1.015,\n    va='top'\n)\n\n# subtitle\nplt.figtext(x=ax[0].get_position().x0+0.03, y=ax[0].get_position().y1*0.925, s=\"GDPR Fine Distributions Over Time (Log Scale)\", va=\"bottom\", ha=\"left\", size=12)\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows an immediate curiosity: several fines are labelled as being given in 1970. It looks like this was this data’s way of encoding NA for the date of fine.\nThe other boxes only vary slightly. They all show Q1 above 1000, Q2 (median around 10000) and Q3 between 10000 and 100000. One notable exception is 2022 which is lower on the y axis.\nLooking at the number of data points per year it looks like 2022 had more GDPR fines than any other year.\nThis report was conducted late in 2023. While this may explain why there are fewer points on the 2023 box, it still looks like there will be fewer fines than in previous years.\nIt doesn’t look as if there is a clear trend in the distribution of fines.\n\n\n\n2.1.2 Controllers\n\nWho pays the most?\nWho pays the most often?\n\nSome data wrangling is required to create a summary view for each controller. Ideally for each separate controller it would be good to construct a table with:\n\nthe controller name\nthe number of times that controller has been fined\nthe sum of the fines that controller has had to pay\nthe mean of the fines that controller has had to pay\n\nThis is useful for direct comparison.\nThe large companies (e.g. Meta, Amazon) often have multiple different controller names. For simplicity in communication these have been combined (i.e. Meta/Facebook refers to any controller regarding Meta/Facebook: Meta Platforms Inc. (Facebook), Meta Platforms Ireland Limited (Facebook), etc.)\n\n\nCode\ndef strfind(series, term):\n    \"\"\"\"\n    time-saver function\n    \"\"\"\n    return series.str.contains(term, case=False)\n\n# Give multi-national corps standard names\n# Facebook/Meta Ireland -&gt; Facebook/Meta\nfines_controller_gb = (\n    fines_nozero\n    .assign(\n        shortname = lambda x: np.select(\n            [strfind(x.controller, \"Facebook\"), strfind(x.controller, \"Meta \"), strfind(x.controller, \"Amazon\"),\n             strfind(x.controller, \"Google\"), strfind(x.controller, \"Microsoft\"), strfind(x.controller, \"Vodafone\"),\n             strfind(x.controller, \"WhatsApp\"), strfind(x.controller, \"Clearview\"), strfind(x.controller, \"H&M\"),\n             strfind(x.controller, \"Marriott\"), strfind(x.controller, \"Telefonica Moviles\")],\n            [\"Facebook/Meta\", \"Facebook/Meta\", \"Amazon\",\n             \"Google\", \"Microsoft\", \"Vodafone\",\n             \"WhatsApp\", \"Clearview AI\", \"H&M\",\n             \"Marriot\", \"Telefonica Moviles Espana\"],\n            x.controller\n        )\n    )\n    .assign(shortname = lambda x: x.shortname.str.title().str.strip().replace(\"\"))\n    .groupby('shortname')\n)\n\ncontroller_counts = fines_controller_gb.size().to_frame(name='counts')\ncontroller_stats = (\n    controller_counts\n    .join(fines_controller_gb.agg({'price':'sum'}).rename(columns={'price':'total_price'}))\n    .join(fines_controller_gb.agg({'price':'mean'}).rename(columns={'price':'mean_price'}))\n    .reset_index()\n)\n\ncontroller_stats.sort_values('total_price', ascending=False).head(10)\n\n\n\n\n\n\n\n\n\nshortname\ncounts\ntotal_price\nmean_price\n\n\n\n\n411\nFacebook/Meta\n7\n2337051000\n3.338644e+08\n\n\n48\nAmazon\n3\n748020000\n2.493400e+08\n\n\n1096\nWhatsapp\n2\n230500000\n1.152500e+08\n\n\n470\nGoogle\n7\n215600028\n3.080000e+07\n\n\n236\nClearview Ai\n5\n74200000\n1.484000e+07\n\n\n485\nH&M\n1\n32258708\n3.225871e+07\n\n\n1006\nTim - Telecom Provider\n1\n27800000\n2.780000e+07\n\n\n386\nEnel Energia S.P.A.\n1\n26500000\n2.650000e+07\n\n\n172\nBritish Airways\n1\n22046000\n2.204600e+07\n\n\n645\nMarriot\n1\n20450000\n2.045000e+07\n\n\n\n\n\n\n\n\nWho pays the most?\nWith the data in this form (each row being a single controller) and the variables relating to statistics on their fines, the controllers with the highest total amount in fines paid can be found easily. First by sorting the data based on total_price, then by visualising those totals for the top n rows. In this case n = 10.\n\n\nCode\nTOP_N = 10\n\nfig, ax = plt.subplots()\n\nsns.barplot(y='shortname', x='total_price', data=controller_stats.nlargest(TOP_N, columns='total_price'),\n            color='midnightblue', ax=ax, orient='h', alpha=0.8)\n\nax.set(xlabel=\"\\nTotal Amount Paid in Fines\", ylabel=None)\nax.xaxis.set_major_formatter(big_currency)\n\n# wrap long labels\nf = lambda x: textwrap.fill(x.get_text(), 20)\nax.set_yticklabels(map(f, ax.get_yticklabels()))\n\nfig.canvas.draw()\nax.set_title(label=f\"Total Amount Paid in Fines Controllers (Top {TOP_N})\", loc=\"left\", ha=\"left\", size=12)\nfig.suptitle(\n    t=\"Who Pays the Most?\", ha='left', fontsize=16,\n    x=ax.get_position().x0,\n    y=ax.get_position().y1*0.975\n    )\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWhat should be immediately clear is that Facebook/Meta have paid such large amounts in GDPR-violation fines that it becomes difficult to assess the scale of other companies’ violations.\nAccording to this data, Facebook/Meta have had to pay: 2,337,051,000in fines.\n\n\nWho Pays the Most Often?\nFrom the collapsed controller statistics the number of recorded violations is immediately available for plotting too. This is the number of times each controller has been found in violation of one or more GDPR articles and has been fined.\nOne note: some controller names don’t relate to companies or are uninformative (“Unknown Company”, “Not Available”). These have therefore been removed before plotting.\n\n\nCode\n# some controller names are uninformative\nDONT_INCLUDE = [\"Unknown\", \"Unknown Company\", \"Not Available\"]\n\nfig, ax = plt.subplots()\n\nsns.barplot(\n    y='shortname', x='counts',\n    data=controller_stats.query(f'~shortname.isin({DONT_INCLUDE})').nlargest(n=TOP_N, columns='counts'),\n    color='midnightblue', ax=ax, orient='h', alpha=0.8\n    )\n\nax.set(\n    xlabel=\"\\nn Times Fined\", ylabel=None,\n    xticks=range(0, controller_stats.counts.max(), 10)\n    )\n\n# wrap long labels\nf = lambda x: textwrap.fill(x.get_text(), 20)\nax.set_yticklabels(map(f, ax.get_yticklabels()))\n\nfig.canvas.draw()\n\n# title and subtitle\nax.set_title(\n    label=f\"Number of Times Each Controller has been Fined (Top {TOP_N})\", loc=\"left\", ha=\"left\", size=12)\nfig.suptitle(\n    t=\"Who Pays the Most Often?\", ha='left', fontsize=16,\n    x=ax.get_position().x0,\n    y=ax.get_position().y1*0.975\n    )\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThe plot shows that the most frequently-fined controller is “Private Individual”. Since this more likely relates to many different individuals, it’s also probably worth looking at the results without these non-specific identifiers. I.e. without terms like: “Private Individual”, “Company”, “Store Owner”, “Police Officer”, …\n\n\nCode\n# some controller names don't relate to specific companies or are uninformative\nDONT_INCLUDE = [\"Unknown\", \"Unknown Company\", \"Not Available\", \"Not Disclosed\", \"Private Individual\", \"Company\", \"Homeowners Association\", \"Store Owner\", \"Police Officer\", \"Website Operator\", \"Bank\", \"Employer\", \"Physician\", \"Restaurant\", \"Retailer\", \"Covid-19 Test Center\", \"Private Person\", \"..\", \"Political Party\", \"Telecommunication Service Provider\"]\n\nTOP_N = 10\n\nfig, ax = plt.subplots()\n\nsns.barplot(\n    y='shortname', x='counts',\n    data=controller_stats.query(f'~shortname.isin({DONT_INCLUDE})').nlargest(n=TOP_N, columns='counts'),\n    color='midnightblue', ax=ax, orient='h', alpha=0.8\n    )\n\nax.set(\n    xlabel=\"\\nn Times Fined\", ylabel=None,\n    xticks=range(0, controller_stats.counts.max(), 10)\n    )\n\n# wrap long labels\nf = lambda x: textwrap.fill(x.get_text(), 20)\nax.set_yticklabels(map(f, ax.get_yticklabels()))\n\nfig.canvas.draw()\n\n# title and subtitle\nax.set_title(\n    label=f\"Number of Times Each Controller has been Fined (Top {TOP_N})\", loc=\"left\", ha=\"left\", size=12)\nfig.suptitle(\n    t=\"Who Pays the Most Often?\", ha='left', fontsize=16,\n    x=ax.get_position().x0,\n    y=ax.get_position().y1*0.975\n    )\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nThis is a bit cleaner. It’s clear that apart from 3, controllers typically haven’t been fined more than 10 times. Vodafone are the surprise here, with over 80 incidences of GDPR violations.\nDuring the filtering the following were excluded but they are of interest that these ‘classes’ of organisation are often found in violation of GDPR. In descending order:\n\n\nCode\nINCLUDE = [\"Private Individual\", \"Company\", \"Homeowners Association\", \"Store Owner\", \"Police Officer\", \"Website Operator\", \"Bank\", \"Employer\", \"Physician\", \"Restaurant\", \"Retailer\", \"Covid-19 Test Center\", \"Private Person\", \"Political Party\", \"Telecommunication Service Provider\"]\n\n(\n    controller_stats\n    .query(f'shortname.isin({INCLUDE})')\n    .sort_values('counts',  ascending=False)\n)    \n\n\n\n\n\n\n\n\n\nshortname\ncounts\ntotal_price\nmean_price\n\n\n\n\n801\nPrivate Individual\n98\n167250\n1706.632653\n\n\n258\nCompany\n25\n306323\n12252.920000\n\n\n500\nHomeowners Association\n16\n22420\n1401.250000\n\n\n958\nStore Owner\n12\n14140\n1178.333333\n\n\n783\nPolice Officer\n9\n13428\n1492.000000\n\n\n1094\nWebsite Operator\n8\n18550\n2318.750000\n\n\n133\nBank\n7\n151870\n21695.714286\n\n\n384\nEmployer\n6\n34211\n5701.833333\n\n\n771\nPhysician\n6\n13200\n2200.000000\n\n\n856\nRestaurant\n6\n19470\n3245.000000\n\n\n861\nRetailer\n6\n17267\n2877.833333\n\n\n299\nCovid-19 Test Center\n5\n23300\n4660.000000\n\n\n804\nPrivate Person\n5\n15600\n3120.000000\n\n\n786\nPolitical Party\n4\n24300\n6075.000000\n\n\n988\nTelecommunication Service Provider\n4\n432213\n108053.250000\n\n\n\n\n\n\n\n\n\n\n2.1.3 Article References\nAt present, the clean fines data is structured such that each row represents a single instance of an entity being fined in violation of one or more GDPR articles. It’s possible for a single fine to be in reference to multiple articles being violated. To look at specific article violations (i.e. one row per article violation) the data will need to pivoted longer. After the data have been put into a longer format, each individual article can be counted using pandas pd.DataFrame.value_counts().\n\n\nCode\nfines_long = pivot_fines_longer(fines_nozero)\n\n# df showing article next to how often it was referenced.\nn_citations = (\n    fines_long\n    .value_counts('article_number')\n    .reset_index(name='count')\n    .sort_values('article_number')\n)\n\n\n\nWhich articles are referenced the most often?\nThere are over 50 different article numbers recorded. Looking at only articles refernced at least 30 times:\n\n\nCode\nMIN_OCCURENCES = 30\n\nfig, ax = plt.subplots()\n\nsns.barplot(\n    x='article_number', y='count',\n    data=n_citations.query(f'count &gt;= {MIN_OCCURENCES}'),\n    orient='v', color='grey', ax=ax\n    )\n\nax.tick_params(axis='y', which='major')\nax.set(\n    xlabel='\\nArticle Number',\n    ylabel='Number of Associated Violations/Fines\\n'\n    )\n\nfig.canvas.draw()\n\n# title + subtitle\nax.set_title(f'At least than {MIN_OCCURENCES} instances', loc='left', fontsize=12)\nfig.suptitle('Number of Fines by Article Number', x=ax.get_position().x0, ha='left', fontsize=16, y=ax.get_position().y1*0.975)\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()\n\n\n\n\n\n\n\n\n\nWe can see from the barplot that articles 5, 6, 13, and 32 are the most frequently referrenced articles (all over 200 instances). With 5 being the most frequently referrenced article.\n\n\nDistribution of Fine Price per Article\n\n\nCode\nMIN_OCCURENCES = 30\n\ndf = n_citations.query(f'count &gt;= {MIN_OCCURENCES}').merge(fines_long, how='inner', left_on='article_number', right_on='article_number')\n\nfig, ax = plt.subplots()\n\nsns.stripplot(\n    x='article_number', y='total_fine_euro', data=df,\n    color='black', jitter=0.15, size=3.5, alpha=0.05,\n    ax=ax, zorder=1\n)\n\nsns.boxplot(\n    x='article_number', y='total_fine_euro', data=df,\n    ax=ax,\n    showfliers=False,\n    boxprops={'facecolor':'none'}\n)\n\nax.set_yscale(\"log\")\nax.set(xlabel=\"\\nArticle Number\", ylabel=\"Total Fine (€) [Logarithmic Scale]\\n\")\nax.yaxis.set_major_formatter(big_currency)\n\nfig.canvas.draw()\n\n# title + subtitle\nax.set_title(f'At least than {MIN_OCCURENCES} instances', loc='left', fontsize=12)\nfig.suptitle('Distribution of Total Fine per Article\\'s inclusion', x=ax.get_position().x0, ha='left', fontsize=16, y=ax.get_position().ymax*0.975)\n\nax.grid(visible=True, axis=\"both\")\n\nfig.patch.set_linewidth(1)\nfig.patch.set_edgecolor(\"black\")\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>GDPR Fines Data Exploration</span>"
    ]
  },
  {
    "objectID": "dev.html",
    "href": "dev.html",
    "title": "3  Regression Analysis",
    "section": "",
    "text": "4 TODO:: return to -&gt; decide to delete\ngdpr_model_data = ( overmin_incidence .assign(violated=1, year = lambda x: x.date.dt.year, total_fine_euro = lambda x: np.log10(x.total_fine_euro)) .loc[:, [‘id’,‘year’, ‘country’, ‘total_fine_euro’, ‘article_number’, ‘violated’]] .query(‘year &gt; 2000’) .pivot_table(values=[‘violated’], index=[‘id’, ‘year’, ‘country’, ‘total_fine_euro’, ‘article_number’]) .unstack(level=-1, fill_value=0) )\ngdpr_model_data.columns = gdpr_model_data.columns.droplevel(0) gdpr_model_data.columns.name = None\ngdpr_model_data.columns = [‘A’ + str(col) for col in gdpr_model_data.columns]\ngdpr_model_data = gdpr_model_data.reset_index()\nCode\nimport statsmodels.api as sm\nCode\ngdpr_model_data.describe(include='all').transpose()\n\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nid\n1564.0\n833.065217\n489.528139\n1.000000\n408.75000\n821.500000\n1252.250000\n1701.000000\n\n\ntotal_fine_euro\n1564.0\n4.026971\n1.007934\n1.447158\n3.30103\n3.845098\n4.645893\n9.079181\n\n\nyear\n1564.0\n2020.564578\n5.120960\n1970.000000\n2020.00000\n2021.000000\n2022.000000\n2023.000000\n\n\nA2\n1564.0\n0.026854\n0.161709\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA5\n1564.0\n0.527494\n0.499403\n0.000000\n0.00000\n1.000000\n1.000000\n1.000000\n\n\nA6\n1564.0\n0.350384\n0.477243\n0.000000\n0.00000\n0.000000\n1.000000\n1.000000\n\n\nA7\n1564.0\n0.023657\n0.152028\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA9\n1564.0\n0.069054\n0.253627\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA12\n1564.0\n0.111893\n0.315335\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA13\n1564.0\n0.196292\n0.397319\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA14\n1564.0\n0.042199\n0.201108\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA15\n1564.0\n0.069054\n0.253627\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA17\n1564.0\n0.034527\n0.182636\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA21\n1564.0\n0.038363\n0.192133\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA24\n1564.0\n0.028133\n0.165406\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA25\n1564.0\n0.043478\n0.203996\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA28\n1564.0\n0.033887\n0.180997\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA31\n1564.0\n0.021739\n0.145877\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA32\n1564.0\n0.222506\n0.416062\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA33\n1564.0\n0.034527\n0.182636\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA35\n1564.0\n0.023657\n0.152028\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\n\n\nA58\n1564.0\n0.040281\n0.196681\n0.000000\n0.00000\n0.000000\n0.000000\n1.000000\nCode\nX = gdpr_model_data.drop(columns=[\"id\", \"total_fine_euro\"])\ny = gdpr_model_data.total_fine_euro\n\nmod = sm.OLS(y, X.assign(constant=1).astype('float'))\nres = mod.fit()\n\ncoefs = pd.concat([res.params, res.conf_int()], axis=1)\n\ncoefs.columns = ['estimate', 'ci0', 'ci1']\nCode\n# make a nice coeficient table for plotting\ncoefTable = (\n    coefs\n    .reset_index(names='coef')\n    .assign(\n        ci_length = lambda x: x.ci1 - x.estimate,\n        hits0     = lambda x: (0 &gt; x.ci0) & (0 &lt; x.ci1)\n    )\n    .loc[lambda x: x.coef != 'constant', ['coef', 'estimate', 'ci_length', 'hits0']]\n)\nCode\ncoefTable.head(2)\n\n\n\n\n\n\n\n\n\ncoef\nestimate\nci_length\nhits0\n\n\n\n\n0\nyear\n-0.001622\n0.009011\nTrue\n\n\n1\nA2\n-0.456740\n0.297996\nFalse\nCode\ncolors = {'True': 'indianred', 'False': 'steelblue'}\n\nfig, ax = plt.subplots()\n\nax.axhline(y=0, color=\"black\", linestyle=(0, (1, 1)))\nax.set(ylabel=None, xlabel=\"Coefficient Estimate\", title=\"Coefficient Estimates\", xticks=coefTable.index, xticklabels=coefTable.coef)\n\nfor hit_type in np.unique(coefTable.hits0):\n    color = colors[str(hit_type)]\n    df_subset = coefTable.query(f'hits0 == {hit_type}')\n    ax.errorbar(\n        df_subset.index, df_subset.estimate, yerr=df_subset.ci_length,\n        marker='o', color=color, ls='', ms=5, capsize=5, label=hit_type\n    )\n\nplt.show()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "dev.html#creating-a-dataset-for-modelling",
    "href": "dev.html#creating-a-dataset-for-modelling",
    "title": "3  Regression Analysis",
    "section": "3.1 Creating a dataset for Modelling",
    "text": "3.1 Creating a dataset for Modelling\n\n\nCode\n\nMIN_CITATIONS = 30\nINTEREST_VARS = ['year', 'article_number']\n\nindex = ['id', 'total_fine_euro'].extend(INTEREST_VARS)\n\nprint(index)\n\n# pivot data\ngdpr_model_data = (\n    fines\n    .assign(\n        violated=1,\n        total_fine_euro = lambda x: np.log10(x.total_fine_euro),\n        year            = lambda x: x.date.dt.year\n    )\n    .merge(\n        n_citations.query(f'count &gt;= {MIN_CITATIONS}'),\n        how='inner', left_on='article_number', right_on='article_number'\n        )\n    .pivot_table(values=['violated'], index=['id', 'total_fine_euro'] + INTEREST_VARS)\n    .unstack(level=-1, fill_value=0)\n)\n\n# sort out indexing\ngdpr_model_data.columns = gdpr_model_data.columns.droplevel(0)\ngdpr_model_data.columns.name = None\n\ngdpr_model_data.columns = ['A' + str(col) for col in gdpr_model_data.columns]\n\ngdpr_model_data = pd.get_dummies(gdpr_model_data.reset_index())\n\n\nNone\n\n\n\n\nCode\ngdpr_model_data\n\n\n\n\n\n\n\n\n\nid\ntotal_fine_euro\nyear\nA2\nA5\nA6\nA7\nA9\nA12\nA13\n...\nA17\nA21\nA24\nA25\nA28\nA31\nA32\nA33\nA35\nA58\n\n\n\n\n0\n1\n3.972203\n2019\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n1\n0\n0\n0\n0\n0\n\n\n1\n2\n3.397940\n2019\n0\n1\n1\n0\n0\n1\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n2\n3\n4.778151\n2019\n0\n1\n1\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n3\n4\n3.903090\n2019\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n1\n0\n0\n0\n0\n\n\n4\n5\n5.176091\n2019\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n1559\n1696\n4.095169\n2022\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n0\n1\n0\n0\n\n\n1560\n1698\n3.301030\n2020\n0\n0\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1561\n1699\n3.397940\n2020\n0\n1\n0\n0\n0\n0\n0\n...\n0\n0\n0\n0\n0\n0\n1\n0\n0\n0\n\n\n1562\n1700\n4.301030\n2020\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n1563\n1701\n3.602060\n2020\n0\n0\n0\n0\n0\n0\n1\n...\n0\n0\n0\n0\n0\n0\n0\n0\n0\n0\n\n\n\n\n1564 rows × 22 columns",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  },
  {
    "objectID": "dev.html#creating-a-dataset-for-modelling-1",
    "href": "dev.html#creating-a-dataset-for-modelling-1",
    "title": "3  Regression Analysis",
    "section": "3.2 Creating a Dataset for Modelling",
    "text": "3.2 Creating a Dataset for Modelling",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Regression Analysis</span>"
    ]
  }
]